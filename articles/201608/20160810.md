# DeepLearning(1): まずは順伝播

さて、今回からは新たなネタとしてDeepLearningに取り組もう。
DeepLearningといえばすでにブームは去って、今は現実の課題に
どんどん活用されている状況だ。使いやすく性能のいいツールや
ライブラリがいろいろあり、ちょっと学べば簡単にその恩恵に
与ることができる（かもしれない）。

しかし個人的には、仕組みがわからないのをただ使うのは面白くないし
個人的に写真の自動分類をしたいというニーズもあるし、遅まきながら
自分で作ってみよう、というわけだ。

## 目標と参考資料

最終的には「一般物体認識」まで行きたい（というかそうでないと
写真の分類にならない）が、目標が高すぎると頓挫するのでこの
記事では以下の2つのステップまでとする。

* ステップ1: yusugomoriさんの[python実装](https://github.com/yusugomori/DeepLearning/tree/dev)を写経する
* ステップ2: 上を使ってお決まりの[MNISTデータ](http://yann.lecun.com/exdb/mnist/)を使った性能検証をする

「自分で作ってみよう」と大口を叩いていながら他の方の実装を写経する
とは詐欺的だが、そこは当方素人なので。。。以後、yusugomoriさんのpython実装を
「yusugomori実装」と表記する。

今回作るものは、畳み込みニューラルネットワーク(Convolutional Neural
Network, CNN)による画像認識プログラムとする。以下、参考にした書籍や
Webサイトを列挙する。

* 書籍
* Webサイト

今回のソースは[こちら](https://github.com/eijian/deeplearning/tree/version-0.1.0.1)

## まずは型を考える

（ちゃんと理解できているとは言い難いが）CNNによる画像認識は
次の図のように「画像」を各レイヤで変換していって最終的に分類結果
（どれに該当するかを確率で示す）を得るようだ。

（図）

そこでまずは「画像(Image)」型と「レイヤ(Layer)」型を定義しよう。
カラー画像だと、XY二次元のドットで構成された平面(Plain)が
赤緑青の3色（チャネルとする）集まってできている。一般の画像ファイル
では各ドットを0-255の整数で表すことが多いが、CNNでは強さを0.0-1.0の
実数で表すようだ。畳み込み層の変換後のデータも多チャンネルの「画像」と
みなせそうなので、`Image`を画像は次のように定義した。実態は`Double`の
3次元リストだ。

```haskell(Image.hs)
type Plain = [[Double]]    -- 2D: X x Y pixels
type Image = [Plain]       -- n channel
```

わざわざ`Image`を`Plain`のリストにしたのは、`Plain`ごとに処理することが
多そうだからだ。
あとついでに、「学習」の際に必要となる教師データも定義しておく。

```haskell(Image.hs)
type Class = [Double]    -- trainer vector
type Trainer = (Image, Class)
```

実数のリストだが、教師データは要素のどれか一つが1.0、他が0.0となる。

次にレイヤを考える。CNNでは次のようなレイヤが使用される。

* 畳み込み層 (convolution layer)
* プーリング層 (pooling layer)
* 活性化層/活性化関数 (activation layer)
* 全結合層 (fully connected layer)

このうち、活性化層というのは一般的な表現かどうかよくわからない。
詳細は後述する。

これらを総称してレイヤ型としたい。最初は型クラスとして`Layer`を
定義し各層を型としてやればいいと考えた。が、後述するが各層をつなげて
一連の変換処理を表現するのに「リスト」を使いたかったので
（私の知識範囲では）うまい方法が思いつかなかった。ということで、
`Layer`型を次のように定義することにした。

```haskell(LayerType.hs)
data Layer = NopLayer
           | ConvLayer Int [FilterC]
           | MaxPoolLayer Int
           | ActLayer ActFunc
           | HiddenLayer [FilterH]
           | FlattenLayer (Image -> Image)
```

ここで、`NopLayer`は何も変換しないダミーのレイヤ（テスト用）、
プーリング層は実用上Max Poolingだけでよいと考えて`MaxPoolLayer`、
全結合層は隠れ層ということで`HiddenLayer`としている。
（対応付けがわかりにくいが）
各定義中に出てくるよくわからない型はそれぞれ以下のように定義
してある。`ActFunc`は活性化関数、`FilterC`は畳み込み層のための
フィルタ、`FilterH`は全結合層(隠れ層)のためのフィルタ、をそれぞれ
表す型だ。

```haskell(LayerType.hs)
-- for Activation Layer
type ActFunc = [Double] -> [Double]

-- filter for Convolution Layer
type Kernel = [[Double]]
type Bias   = Double
type FilterC = (Kernel, Bias)

-- filter for Hidden Layer
type FilterH = [Double]

```

## 学習用プログラム

deep learningで画像認識させるためのステップは大雑把には次のような
手順を踏む必要があると思っている。

1. 大量のサンプル画像を用意し、それらを分類する。
2. サンプル画像から「教師データ」を作成する。
3. 教師データを使って「学習」を行う。学習とは実際は幾つかの層で
   使われるフィルタを教師データを元に調整していくこと。
4. 画像を調整済みフィルタを使って分類する（各分類の可能性を計算する）。

1と2はプログラム以前の準備段階だ。が、yusugomori実装では教師データと
学習状況の確認につかうテストデータをプログラム内で生成している。
最終的には学習データとテストデータは外から与える形にしたいが、
まずは同じようにプログラム内で生成するようにしよう。

### 教師データの生成

ゆくゆく教師データを外から与えることも考え、「画像データの集合」
（プールとする）の型を作ろう。これらの集合から幾つかを取り出して
学習に使いたい。そこで、データに追番をつけて`Map`に登録
することにした。とりあえずはオンメモリで処理できれば良いので
以下のようにプール（オンメモリ版、`MemPool`）を定義した。

```haskell(Pool.hs)
class Pool p where
  getImages :: p -> Int -> Int -> IO [Trainer]
  nSample :: p -> Int

newtype MemPool = MemPool { m :: Map.Map Int Trainer }
```

yusugomori実装に倣って教師データを生成する関数も用意した。

```haskell(Pool.hs)
initSamplePool :: Int -> (Int, Int) -> Int -> Double -> Int -> IO MemPool
initSamplePool c (sx, sy) o p n = do
  s0 <- forM [0..(n-1)] $ \i -> do
    let cl = i `mod` o  -- class of this image

    -- Image data
    s1 <- forM [1..c] $ \j -> do
      s2 <- forM [0..(sy-1)] $ \y -> do
        let p' = if y `div` st == cl then p else (1-p)
        s3 <- forM [1..sx] $ \x -> do
          a <- pixel p'
          return a
        return s3
      return s2

    -- Trainer data
    e1 <- forM [0..(o-1)] $ \j -> do
      return $ if j == cl then 1.0 else 0.0
    return (s1, e1)

  return (MemPool (Map.fromList $ zip [0..] s0))
  where
    st = sy `div` o
    pixel :: Double -> IO Double
    pixel p = do
      v <- MT.randomIO :: IO Double
      let v' = if v < p then 0.5 else 0.0
      return v'
```

結構ごちゃごちゃしているが、以下のような3種類の画像データを生成してプール
として返しているのだ。

（画像サンプル）

実際は上記のようなキッチリした画像ではなく、灰色部分に一定割合で白が
混ざっていて、白色部分は逆に灰色が混ざっている。教師データではその
割合を5%、テストデータは10%としている。

つぎに、プールから指定枚数の画像を取り出す関数`getImages`とプール内の
画像総数を返す関数`nSample`を示そう。

```haskell(Pool.hs)
{-
getImages
  IN : pool
       epoch number
       batch size
-}

instance Pool MemPool where
  getImages p@(MemPool m) e b = do
    let s = nSample p
        o = (e-1) * b `mod` s
        mx0 = o + b - 1
        mx2 = mx0 - s
        mx1 = if mx2 < 0 then mx0 else s - 1
        im0 = mapMaybe (\x -> Map.lookup x m) [o..mx1]
        im1 = mapMaybe (\x -> Map.lookup x m) [0..mx2]
    return (im0 ++ im1)

  nSample (MemPool m) = Map.size m
```

`getImages`は若干ややこしいが、要は教師データを満遍なく学習させたいので
epoch毎に違う画像を取り出すようにしている（最初のepochで1番目から10個
取り出したら次のepochでは11番目から10個取り出す）。
ゆくゆくは順番に取り出すのではなくランダムに取り出すオプションも実装したい。

### main関数（全体の流れ）

ここで、全体の流れを説明しておきたいので`main`関数の話をしよう。
大雑把には次のような流れだ。

* パラメータ定義(`main`以前)：フィルタの大きさとかバッチサイズとか
  （ゆくゆくはパラメータを設定ファイルから読み込むようにしたい）
* 教師データ、テストデータの準備
* 各レイヤの初期フィルタの生成
* レイヤの組み立て
* 学習の繰り返し

まずパラメータの説明から。画像は3種類に分類する(k)。教師データが
50x3=150個、テストデータが10x3=30個だ。画像サイズ、フィルタは
先に示した各レイヤの仕様どおり。なおバッチサイズは教師データが少ないので
毎回全教師データを学習するために150個としている。

```haskell(Main-cnn.hs)
-- PARAMETERS

-- training/test dataset
k = 3   -- number of class
n = 50  -- number of teacher data for each class
m = 10  -- number of test data for each class

train_N    = n * k
test_N     = m * k

-- image size
image_size = [12, 12]
channel    = 1

-- filter specs
n_kernels    = [10, 20]
kernel_sizes = [3, 2]
pool_sizes   = [2, 2]
n_hidden     = 20
n_out = k

-- loop parameters
epochs = 200
opStep = 5
epoch0 = 1
batch  = 150

-- others
learning_rate = 0.1
```

次にメインルーチンだ。教師データとテストデータは次のように生成
している。説明はいらないだろう。

```haskell(Main-cnn.hs)
main :: IO ()
main = do
  putStrLn "Building the model..."
  poolT <- initSamplePool 1 (12, 12) 3 0.95 train_N   -- trainer data pool
  poolE <- initSamplePool 1 (12, 12) 3 0.90 test_N    -- test data pool
  sampleE <- getImages poolE 1 test_N
```

次はレイヤの定義だ。まずフィルタを初期化してからレイヤを複数並べる。

```haskell(Main-cnn.hs)
  fc1 <- initFilterC 10 1 12 12 3 2
  fc2 <- initFilterC 20 10 5 5 2 2
  fh1 <- initFilterH n_hidden (2*2*20)
  fh2 <- initFilterH n_out n_hidden

  let layers = [ConvLayer 3 fc1, ActLayer relu, MaxPoolLayer 2,
                ConvLayer 2 fc2, ActLayer relu, MaxPoolLayer 2,
                FlattenLayer flatten,
                HiddenLayer fh1, ActLayer relu,
                HiddenLayer fh2, ActLayer softmax]
```

ここでは、畳み込み層1（活性化関数ReLU)→プーリング層1→
畳み込み層2(活性化関数ReLU)→プーリング層2→隠れ層(活性化関数ReLU)→
出力層(活性化関数softmax) という多層構造にした。
なお、`FlattenLayer`は畳み込み+プーリングの処理から全結合へデータ形式を
変換する処理である。

最後が学習の繰り返しだ。

```haskell(Main-cnn.hs)
  tm0 <- getCurrentTime
  putStatus 0 tm0 [([0.0], 0.0)] layers
  loop is tm0 layers batch poolT sampleE
```

時間の記録と学習前の状態をダミーで表示したあと、繰り返し学習する
`loop`を呼んでいる。`loop`の詳細は以下。

```haskell(Main-cnn.hs)
loop :: Pool p => [Int] -> UTCTime -> [Layer] -> Int -> p -> [Trainer]
     -> IO ()
loop [] _ _ _ _ _ = putStr ""
loop (i:is) tm0 ls b pt se = do
  teachers <- getImages pt i b
  ops <- mapM (train ls) teachers
  let (_, dls) = unzip ops
      ls' = updateLayer ls dls   -- dls = diff of layers
  if i `mod` opStep == 0 then putStatus i tm0 (evaluate ls' se) ls'
                         else putStr ""
  loop is tm0 ls' b pt se
```

大したことはやってなくて、epoch毎に繰り返し`loop`を呼んでいて、
epoch番号がなくなったら(空リスト)、繰り返しが終了する。
中身は、学習する教師データをプールから取り出して、それぞれ学習をし、
各層を学習結果を使って更新する。とはいえ、今回は「学習」は
実装しておらず、この部分はダミーである。

ここまでがmain関数だ。

## 各層の順伝播処理

`loop`の中で呼び出している`train`が学習の実態だが、中身は各層の
順伝播処理を呼んでいるだけなので簡単だ。

```haskell(Trainer.hs)
train :: [Layer] -> Trainer -> IO (Image, [Layer])
train [] (i, c) = return (i, [])
train ls (i, c) = do
  let op  = forwardProp ls [i]
      ls' = backwordProp ls op
  return (head op, ls')
```

`forwardProp`が順伝播、`backwordProp`が逆伝播だが、今回は順伝播のみ
実装した。その実体は再帰によりリスト内の層を順に適用している。
さらに、層毎にその独自処理を呼び出すようにした。

```haskell(Layer.hs)
forwardProp :: [Layer] -> [Image] -> [Image]
forwardProp [] is = is
forwardProp (l:ls) is = forwardProp ls (forward l is)

forward :: Layer -> [Image] -> [Image]
forward _ [] = []
forward NopLayer i = i
forward l@(ActLayer f) im@(i:is)     = (activate l i):im
forward l@(MaxPoolLayer s) im@(i:is) = (poolMax l i):im
forward l@(ConvLayer s fs) im@(i:is) = (convolve s fs i):im
forward l@(HiddenLayer fs) im@(i:is) = (connect fs i):im
forward l@(FlattenLayer f) im@(i:is) = (f i):im
```

以下、各レイヤでの処理を説明する。

### 何もしない層(NopLayer)

これは上記の通り、入力をそのまま返している。プログラムの動作を確認する
ために用意したダミーだ。何も言うことはない。

### 畳み込み層(ConvLayer)


### 活性化層/活性化(ActLayer)

先にも書いた通り、ニューラルネットについての書籍やサイトでは活性化層という
独立した層として説明されておらず、畳み込み層や隠れ層の後処理的な扱いだ。
が、今回は独立した層としたほうが「私にとって」わかり易そうだったので
分けることにした。後でチューニングのために他の層に統合するかもしれない・・・。

活性化関数として、まずはReLUとsoftmax関数を用意した。それぞれ下記で表される。
これらはActLayer層の作成時にどちらか選ぶのだ。

(ReLU)
$
f(x) = max(x, 0)
$

(Softmax)
$
f(x) =
$

実装はこう。

```haskell(ActLayer.hs)
relu :: ActFunc
relu as = map f as
  where
    f :: Double -> Double
    f a = if a > 0.0 then a else 0.0

softmax :: ActFunc
softmax as = map (\x -> x / sume) es
  where
    amax = maximum as
    es   = map (\x -> exp (x - amax)) as
    sume = sum es

activate :: Layer -> Image -> Image
activate (ActLayer f) im = map (map f) im
```

記事を書いていて、ReLUのほうは素直に`maximum`を使えばいいと思った。
後で直そう。Softmaxの方も、ほぼ定義通りだ。ただし入力`x`に対し、
`x`の最大値を引いているがこれは発散しないための工夫だそうだ。
本体である`activate`は画像のすべての画素に対して活性化関数を適用する。
まあ、難しいものではない。

### プーリング層(MaxPoolLayer)

プーリング層としては最大プーリングのみ対応する。他も必要になったら
その時考えよう。


### フラット化(FlattenLayer)

これは層というよりは整形処理のようなものだ。畳み込み層やプーリング層は
二次元平面が複数チャネルという、いわゆる「画像」の形をしているが、
隠れ層では「一次元配列」を入力に取る。ということでそれらの変換が必要なのだ。
その変換関数は画像と同じところに定義している。

```haskell(Image.hs)
flatten :: Image -> Image
flatten im = [[concat $ concat im]]
```

単に画像の一枚目からが措置を一列に並べているだけである。
ただし、`Layer`として同型の関数で処理するためにはあくまでも
「画像」である必要がある。なのでわざわざ結果を3次元配列にみせかけている。
=の後ろの角括弧2つがそれだ。

### 隠れ層(HiddenLayer)

これはいわゆる全結合層なのだが、他の記事やプログラムを見ると隠れ層と
いう名で作られているのでそれに倣った。隠れ層としながら、最終の出力層も
この`HiddenLayer`を使っているので名前は変えるべきかもしれない。。。

この層では、N個の入力をM個の出力に変換する。Nの要素全てがMの要素全てに
関連付けられているため全結合というらしい。その関連付けをフィルタとして
与える必要がある。初期はランダム値を設定し、学習によってその値を少しずつ
補正することで画像認識などができるらしい。フィルタはN×Mの二次元リストで
与えることにしよう。

```haskell(HiddenLayer.hs)
initFilterH :: Int -> Int -> IO [FilterH]
initFilterH k c = do
  f <- forM [1..k] $ \i -> do
    f' <- initKernel c
    return f'
  return f
  where
    a = 1.0 / fromIntegral c
    initKernel :: Int -> IO FilterH
    initKernel c = do
      w <- forM [1..c] $ \i -> do
        r <- MT.randomIO :: IO Double
        return ((r * 2.0 - 1.0) * a)
      return (0.0:w)
```

プログラム中ではNがc、Mはkで表している。それぞれチャネルとカーネル。
名づけがおかしいかもしれないが。
各要素は入力用素数Nに対し1/N～-1/Nの間の乱数としている。
ただし、最初の要素はバイアス値で、これは0.0とするらしい。

フィルタの処理は次の通り。入力値にフィルタ値を掛けてすべて足しこむ。
式で書くとこうなる。なおフィルタの要素は$w_ij$としている。

$
M_i = \sigma N_j w_ij + bj
$

実装はこう。

```haskell(HiddenLayer.hs)
connect :: [FilterH] -> Image -> Image
connect [] _ = error "invalid FilterH"
connect _ [] = error "invalid Image"
connect fs [[im]] = [[map (dot im) fs]]
  where
    dot :: [Double] -> [Double] -> Double
    dot im f = sum $ zipWith (*) (1.0:im) f
```

## 実行してみた

以上でひとまず順伝播については実装できた（と思う）。コンパイル・実行して
みよう。

```shell
$ cabal configure
  :
$ cabal build
  :
$ dist/build/deeplearning/deeplearning
Building the model...
Training the model...
iter = 0/200 time = 0s ratio = 0.0
iter = 5/200 time = 0.0030001s ratio = 0.33333713817332733
iter = 10/200 time = 0.333019s ratio = 0.33333713817332733
iter = 15/200 time = 0.682039s ratio = 0.33333713817332733
     :
iter = 200/200 time = 13.0807482s ratio = 0.33333713817332733
Finished!
```

出力内容は、左からエポック（反復回数）、経過時間(秒)、正解率だ。
正解率は、全テストデータについて、出力リストから正解にあたる要素の
値を取り出して平均したもの。先の通り3種類に分類されるデータを
使ったので、何も学習していない状況では出力は均等割合となり約1/3に
なっている。まあ、結果の数字だけ見ればなんとなくうまく処理できて
いるように見える。

## まとめ

今回はCNNの画像認識プログラムの初回として以下に取り組んだ。

* CNNで使われるいろいろな型を定義した。
* 入力値を変換するいろいろな「層」を定義した。
* これらを使って順伝播の処理を実装した。

各層の処理を経て、なんとなく結果が出力されたので雰囲気はつかめたかなと。

順伝播だけでは何にも有用な結果は得られないが、それは次回取り組む（だろう）
逆伝播の結果に期待したい。

しかし今回は文章が長くなってしまった。いろいろ下準備とかmain関数とかの
説明が多かったと反省。次回以降は短くまとめられるだろう。
