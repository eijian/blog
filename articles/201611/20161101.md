# DeepLearning(3): そして逆伝播（でも全結合層まで）

第1、2回で順伝播の処理は実現した。今回やっと「学習」処理に入る。
（それにしても時間が取れずまたしてもだいぶ間が空いてしまった。記憶が飛んでいる）

前回までで作成したプログラムでは、多段のニューラルネットワークを構成した。
その終盤には2つの全結合層（隠れ層と出力層）が含まれている。本稿では、
逆伝播による学習で全結合層のフィルタ（ウェイト）を更新する部分について
解説し、学習によりフィルタの分類能力が向上することを確認したい。

## 全結合層の逆伝播処理

### 処理の流れ

逆伝播の理論的なところや数式の詳細は各種書籍やWebサイトを見ていただくとして、
後述のコード実装の説明に必要な事項についてざっと説明したい。
畳み込み層の逆伝播はちょっと複雑っぽいので次回にし、今回は全結合層だけに
しておく。


表と図にしてみた。

(表）
（図）

誤差($\delta$)が各層で計算され、次の層へ伝わっていくイメージが伝わるだろうか?
実装面では、逆伝播処理を始める前に順伝播で生成された各層の出力のリストと
各層を組(タプル)のリストにして(図の点線)、それを再帰処理している。
再帰の際に計算された誤差を次に引き渡していくのだ。
ただし逆伝播の際計算に使う各層は順伝播のときのものではなく「逆変換」する
ものである。例えば全結合層のフィルタは行列として表されるが、逆伝播時は
それを転置した行列を使うし、活性化関数も元の関数の導関数を使う。
（という理解で合っているよね?）

全結合層のように学習対象のフィルタを持つ層では、誤差だけでなく前の層から
伝わってきた誤差を使ってフィルタの差分も求める($\Delta W$)。
この差分を使ってフィルタを更新すればいいのだ。
(もう少し複雑な数式もあるが簡単のため今回はこの式で。)

\[
W \leftarrow W - \eta \Delta W
\]

ここで、 $\eta$ は学習率(learning rate)と呼ばれるパラメータだ。
さて問題は $\Delta W$ をどう求めるかだが、損失関数 $E$ の勾配で表す
ことができるようだ。
全結合層のフィルタ $W$ は入力数 $\times$ 出力数の行列と見ることができる。
その個々の要素を $w_{ij}$ とすると、

\[
w_{ij} \leftarrow w_{ij} - \eta \frac{\partial E}{\partial w_{ij}}
\]

となる。この辺の詳細は各種文献を参照のこと。そうそう、最近次の書籍を
入手した。Pythonを使っているがフルスクラッチでのdeep learning実装が
解説してある。まだ読んでいる途中だが。

（本）

なお本プログラムでは出力層の活性化関数に $softmax$ を使っており、
出力値と正解値の誤差を簡単にするため $E$ には交差エントロピー関数を
採用した。$t$ は正解値(ベクトル)、$y$は出力値(ベクトル)だ。例えば
本プログラムでは画像を3種類に分類するのでベクトルの要素数は3、
仮に教師データが1番目の種類を表すのなら、

\[
t = [1.0, 0.0, 0.0], y = [0.81, 0.14, 0.05]
\]

各文字の右肩の $(n)$ は $n$ 番目の訓練データであることを明示する
ために使った。

\[
E^{(n)} = -\sum_k t_k^{(n)} \ln y_k^{(n)}
\]


これをすべての層に対して遡っていけば、めでたく（一回の）学習が完了するわけだ。


### 実装の説明

(学習ループ)

```haskell:Main-train.hs
main = do
     :
  layers' <- trainLoop getTeachers sampleE putF (learnR st) (layers st) is
     :

trainLoop :: (Int -> IO [Trainer]) -> [Trainer]
  -> (Int -> [Layer] -> [Trainer] -> IO ()) -> Double -> [Layer] -> [Int]
  -> IO [Layer]
trainLoop _ _ _ _ ls []             = return ls
trainLoop getT se putF lr ls (i:is) = do
  ls' <- trainLoop getT se putF lr ls is
  teachers <- getT i
  let
    rls = tail $ map reverseLayer $ reverse ls'    -- fist element isn't used
    dls = map (train ls' rls) teachers
    ls'' = update lr (transpose dls) ls'           -- dls = diff of layers
  putF i ls'' se
  return ls''
```

(レイヤと教師データを与えて誤差ΔWを返す)

```haskell:Trainer.hs
train :: [Layer] -> [Layer] -> Trainer -> [Layer]
train [] _ (i, c) = []
train ls rls (i, c) = dls
  where
    (y, op') = splitAt 1 $ forwardProp ls [i]
    d = (head $ head $ head y) `vsub` c
    (_, dls) = backwardProp (zip (tail op') rls) (d, [])
```

(レイヤタイプ毎にそれぞれの関数を呼び出し)

```haskell:Layer.hs
backwardLayer :: Layer -> Image -> Delta -> (Delta, Layer)
backwardLayer (NopLayer)           _  d = (d, NopLayer)
backwardLayer (ActLayer f)         im d = deactivate f    im d
backwardLayer (MaxPoolLayer s)     im d = depoolMax  s    im d
backwardLayer (ConvLayer s fs)     im d = deconvolve s fs im d
backwardLayer (FullConnLayer fs)   im d = deconnect    fs im d
backwardLayer l@(FlattenLayer x y) im d = (head $ head $ unflatten x y [[d]], l)
```



(ΔWのリストからレイヤを更新： W ← W - ΔW)

```haskell:Trainer.hs
update :: Double -> [[Layer]] -> [Layer] -> [Layer]
update lr _ [] = []
update lr [] ls = ls
update lr (dl:dls) (l:ls) = (updateLayer lr l dl):(update lr dls ls)
```

(出力値とレイヤの組を使って再帰)

```haskell:Trainer.hs
backwardProp :: [(Image, Layer)] -> (Delta, [Layer]) -> (Delta, [Layer])
backwardProp [] (_, ls) = ([], ls)
backwardProp ((im,l):ols) (d, ls) = backwardProp ols (d', l':ls)
  where
    (d', l') = backwardLayer l im d
```

(誤差とレイヤ更新情報の計算)

```haskell:FullConnLayer.hs
deconnect :: [FilterF] -> Image -> Delta -> (Delta, Layer)
deconnect fs im delta = (mmul delta fs', FullConnLayer $ calcDiff delta im')
  where
    fs' = tail fs
    im' = head $ head im

calcDiff :: Delta -> [Double] -> [FilterF]
calcDiff delta im = map (mulImage im') delta
  where
    im' = 1.0:im
    mulImage :: [Double] -> Double -> [Double]
    mulImage im d = map (*d) im'
```

(レイヤ更新)

```haskell:FullConnLayer.hs
updateFullConnFilter :: [FilterF] -> Double -> [Layer] -> Layer
updateFullConnFilter fs lr dl = FullConnLayer fs'
  where
    ms = strip dl
    delta = mscale (lr / (fromIntegral $ length ms))  $ msum ms
    fs' = msub fs delta
```



(活性化関数の逆伝播)

```haskell:ActLayer.hs
deactivate :: ActFunc -> Image -> Delta -> (Delta, Layer)
deactivate f im delta
  | c == [0.0]  = (zipWith (*) delta f', ActLayer relu')
  | c == [1.0]  = ([], ActLayer f)
  | otherwise = ([], ActLayer f)
  where
    c = f [0.0]
    f' = relu' (head $ head im)
```





## 「学習」の評価

プログラムも出来上がったことだし、どれだけ学習するか評価してみよう。
コンパイルと実行は次の通り。

```shell
% cabal build
   :
% ./dist/build/train/train
Initializing...
Training the model...
iter =     0/500 accuracy = 0.3333333333 time = 0.149965s
iter =     5/500 accuracy = 0.3333413554 time = 5.132365s
   :
```

途中経過を 5 epoch 毎に画面に出している。前回から少しフォーマットを
変更し、正答率を「認識精度=accuracy」に改めている。

### 学習結果（ノーマル版）

上記のプログラムを10回実行し、それぞれ認識精度と実行時間を計測した。
結果は以下のグラフの通り。

（グラフ）

点線が各試行時の実測値、太い実線が10回の平均値だ。これを見ると、認識精度の
向上具合は各試行でかなりばらついているのがわかる。また、認識率の向上が
急峻なものと緩慢なものにかなり差がついている。

プログラムでは、学習用データ、テスト用データ、および各フィルタに乱数を
使っているが、それらの初期状態によりこれだけの差が生じてしまっていると
思われる。（学習効果が薄い=うまく特徴が出ない画像群が生成されたとか、
フィルタの初期状態が行けてないとか）

ただ、総じて 500 epoch ぐらい繰り返せば認識精度が9割を超えることも分かった。
まだ畳み込み層の学習をしていない段階でここまで精度が上がるとは思っておらず、
嬉しい誤算だった。

### 学習結果（学習率の差）

先のグラフで、認識精度の向上は最初からではなく50 epochを超えたあたりから
急に上がりだしている。私は素人なのでよくわからないが、いろいろな情報を見ると
学習率をどう設定するかとか、学習進行に応じて学習率を変化させるとか書かれて
いて、重要なパラメータであることはわかった。

このプログラムでは学習率を0.1に固定しているが、初期段階ではこれが小さすぎる
のかもしれない。ということで、学習率を0.2と0.05に変えて試行した結果も出して
みた。(0.2と0.05の試行は5回、その平均をとった）

（グラフ）

予想通りかというとちょっと微妙だが、学習率を大きくした方がより速く認識精度が
向上していることがわかる。ただ、初っ端から急上昇しているわけではないので
もっと大きくしないといけないのだろう。ただ、大きいままで学習を続けると
収束しないという話もある（？）ので、やはり動的に変化させる手法がいくつか
あるようだ。もっとあとになるがそれにも取り組みたい。

で、今回のプログラムはまずyusugomori実装を「写経」することが第一課題なので
学習率は0.1に戻しておこう。

### yusugomori実装との比較

ここまで、学習が進んだ！と喜んできたわけだが、「写経元」のyusugomori実装と
同じことができているのだろうか？

ということでyusugomori実装との比較をしてみたのが次のグラフだ。

（グラフ）

avg-0.1が本プログラム（学習率0.1、試行10回の平均値、青線）、py-cnnが
yusugomori実装（学習率0.1、試行1回、赤線）、py-cnnfcがyusugomori実装で
畳み込み層の学習を抑制した改造版（学習率0.1、試行1回、緑線）だ。
yusugomori実装では乱数のseedが固定されているため毎回同じ結果になるのと、
後で述べるようにめちゃくちゃ処理時間が掛かるのとで試行は1回だけにした。

先述の通り乱数の生成値により試行結果はかなりぶれるのだが、青と緑の線は
350 epoch近辺からほとんど同じような値になっていて（楽観的に見れば）
本プログラムはyusugomori実装と同等な処理ができているのではないかと思う。
一方本来の学習（=畳み込み層も学習）であれば認識精度が急上昇し、かつ
200 epoch ぐらいでも認識精度が98%を超えてくる。本プログラムでは
畳み込み層のフィルタがいつまでたっても初期状態（ランダム）のままなので
致し方ないところだろう。まあ、ここまで性能差があると早く畳み込み層の
学習部分も完成させたいと、モチベーションの向上になる。

次に処理時間について見てみる。200 epoch 実行時の結果は次表のとおりである。
(iMac early 2009, 3.0GHz Core 2 Duo, 4GB memory)

||avg-0.1|py-cnn|py-cnnfc|
|:-:|-:|-:|-:|
|処理時間(s)|157|24838|6152|
|処理性能(epoch/s)|1.273|0.008|0.033|
|認識精度(%)|74.2|98.4|62.9|

yusugomori実装の処理時間が半端ではないことがわかると思う。本プログラムも
コンパイルされたネイティブコードであることを考えると決して十分な処理速度
ではないが、それでもまったく比べものになっていない。

NumPyライブラリのインストールに何か抜けている手順があるのだろうか?
インストール時に何か特別なことをした記憶もないので、以下の記事にあるように
「ちゃんとした」インストールができていないのかもしれない。

(参考1) [Kesin's diary: NumPy, SciPyをちゃんとインストールする](http://kesin.hatenablog.com/entry/20111229/1325174595)

もしくは次の記事のように多重ループと配列要素への直接読み書きが
影響しているのかもしれない。

(参考2) [numpyで明示的にループを書くと極端に遅くなる](http://qiita.com/nonbiri15/items/ef97b84832055ab807fb)

いずれにせよ、当方は今の所Pythonを使う気はないので深入りはしない...

## まとめ

今回は全結合層部分の逆伝播処理を実装した。これにより学習ができるように
なったので、制度はともかく画像認識ができるようになった。ただ、これはまだ
半分。次回は畳み込み層の逆伝播を実装し、プログラムを完成させようと思う。
