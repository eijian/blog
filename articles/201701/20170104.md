# DeepLearning(4): 逆伝播完成？

また時間が経ってしまった。今回は畳み込み層の逆伝播のアルゴリズムを
理解するのに手間取ってしまった。。。気を取り直して、逆伝播の後半戦といこう。

## 1. 残るはプーリング層と畳み込み層

今回作っているプログラムは次のように11層のレイヤで画像を学習・評価している。

|No.|層|実装|
|:-:|:-|:-|
|0|入力|-|
|1|畳み込み層1|×|
|2|活性化層1|○|
|3|プーリング層1|×|
|4|畳み込み層2|×|
|5|活性化層2|○|
|6|プーリング層2|×|
|7|平坦化層|×|
|8|全結合層1|○|
|9|活性化層3|○|
|10|全結合層2|○|
|11|活性化層4(出力)|○|

前回までで全結合層の逆伝播処理は説明したので、それらの処理は実装済み、
残るは×のついている層だ。次節でそれぞれ見ていこう。

その前に、前回のプログラムでは逆伝播において伝播させる誤差($\delta$)を
次のように`Double`の一次元配列(というかリスト)とした。

```haskell
type Delta = [Double]
```

しかしプーリング層や畳み込み層では、誤差は単純な一次元配列にならず、画像と
同じくXY画面がNチャネルという三次元のデータになる。ということで、次のように
改めた。なのですでに実装済みとした部分もこれに合わせて変更してある。

```haskell
type Delta = Image  -- (= [[[Double]]])
```

## 2. 各層の逆伝播処理

それでは各層の逆伝播について、アルゴリズムと実装をそれぞれ見ていこう。

### 2-1. 平坦化(Flatten)層

前回は全結合層までの実装だったので、上記の表で言えば No.11 → No.8 までしか
できていなかった。平坦化層はCNNの醍醐味である畳み込み層へ誤差を伝えるための
重要な転換点だ。だから結構複雑かもと勝手に想像していたが、実装は極めて単純
だった。

```haskell:FlattenLayer.hs
unflatten :: Int -> Int -> Image -> Image
unflatten x y [[ds]] = split y $ split x ds
  where
    split :: Int -> [a] -> [[a]]
    split _ [] = []
    split n ds = d : split n ds'
      where
        (d, ds') = splitAt n ds
```

この層の逆伝播(`unflatten`)処理とは、下層から一次元配列で伝わってきた誤差を
画像形式(=三次元配列)に組み替えること。順伝播で画像形式を一列に並び替えたので
その逆をしている。だから画像のX,Y画素数を引数に与えている。先述の通り
`Delta`型を三次元配列に変更したので、第三引数が微妙な形(`[[ds]]`)になっている。
`ds`は`[Double]`だ。ここでの処理はdsの要素を先頭からX個ずつ切り出し、Y個
揃ったら次のチャネルに移るという具合に変換している。`ds`の要素がなくなるまで
繰り返す。

この関数では、入力されるデータ数がX,Y画素できちんと割り切れることを想定して
いて何のエラー処理もしていない。。。

### 2-2. プーリング層

#### 順伝播での出力値を書き換える

#### 誤差の計算



### 2-3. 畳み込み層

#### 誤差の計算


#### 勾配の計算


#### フィルタ更新



## 3. 評価

それでは早速出来上がったプログラムを実行して学習をさせてみよう。
(ソースは[ここ](https://github.com/eijian/deeplearning/tree/version-0.3.0.0))

```shell
$ cabal clean
$ cabal build
  :
$ ./dist/build/train/train
Initializing...
Training the model...
iter =     0/500 accuracy = 0.3333333333 time = 0.13968s
iter =     5/500 accuracy = 0.3333526481 time = 11.021087s
iter =    10/500 accuracy = 0.3333741834 time = 21.513648s
iter =    15/500 accuracy = 0.3334006436 time = 33.707092s
iter =    20/500 accuracy = 0.3334352183 time = 45.034608s
iter =    25/500 accuracy = 0.3334818706 time = 55.7984s
  :
```

### 3-1. 前回分（全結合層のみ）との比較


### 3-2. メモリリーク！？


実行中の`top`コマンドの出力がこれ。`train`プロセスが大量のメモリを
消費しているのがわかる。繰り返し回数がだいたい180ぐらいのときの状態。

```shell
Processes: 162 total, 3 running, 159 sleeping, 639 threads             00:22:32
Load Avg: 1.99, 1.98, 1.80  CPU usage: 8.48% user, 32.11% sys, 59.40% idle
SharedLibs: 92M resident, 33M data, 6516K linkedit.
MemRegions: 23895 total, 477M resident, 29M private, 103M shared.
PhysMem: 4080M used (1131M wired), 14M unused.
VM: 471G vsize, 623M framework vsize, 71855224(48041) swapins, 73935000(0) swapo
Networks: packets: 631497/460M in, 456172/62M out.
Disks: 2706786/300G read, 1541857/302G written.

PID    COMMAND      %CPU TIME     #TH   #WQ  #PORT MEM    PURG   CMPRS  PGRP
25694  train        81.4 08:49.94 1/1   0    10    1064M- 0B     4044M+ 25694
0      kernel_task  76.5 02:28:47 114/7 0    2     443M-  0B     0B     0
  :
```

Macのメモリ管理についてはよくわかっていないが、`train`プロセスの使用メモリが
1064M、CMPRSが4044Mで、合わせて5100M=5GBほども消費している。繰り返しが300回を
超えた時は10GB以上使っている状態で、慌てて強制終了させたほど。

## 4. まとめ

今回でやっと一通りCNNが
[実装](https://github.com/eijian/deeplearning/tree/version-0.3.0.0)
できた。学習能力も手本としたyusugomori実装と同等のものができた。
しかしながら、上述のとおりメモリを大量に消費するという問題を抱えたままで、
このままでは全く実用に耐えない。

次回はこの問題を解決させたいと思う（できないかも・・・）。
